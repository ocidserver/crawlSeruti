# ğŸ¤– BPS Web Crawler - Multi Crawler System

Automated web crawler system untuk BPS dengan support multiple crawlers (Seruti & Susenas), scheduler, dan database management.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)](https://flask.palletsprojects.com/)
[![Selenium](https://img.shields.io/badge/Selenium-4.15+-orange.svg)](https://www.selenium.dev/)
[![SQLite](https://img.shields.io/badge/SQLite-3-blue.svg)](https://www.sqlite.org/)

---

## ğŸ“‹ Features

### ğŸ¯ Core Features

- âœ… **Multi-Crawler Architecture** - Support Seruti & Susenas crawlers
- âœ… **SSO Authentication** - Auto login via BPS SSO (https://sso.bps.go.id)
- âœ… **Smart Download Detection** - Skip duplicate data berdasarkan tanggal
- âœ… **Task Scheduler** - Scheduled jobs dengan cron-like scheduling
- âœ… **SQLite Database** - ACID-compliant data storage
- âœ… **Headless Mode** - Background execution tanpa GUI

### ğŸ“Š Management Features

- âœ… **Job History** - Track semua jobs (active, completed, cancelled, failed)
- âœ… **Download Log** - Complete download tracking dengan task name
- âœ… **Retry Mechanism** - Auto-retry dengan configurable delay
- âœ… **Web Dashboard** - Modern UI dengan table format
- âœ… **Real-time Status** - Live job monitoring

### ğŸ”’ Data & Security

- âœ… **Transaction Safety** - No data loss saat system crash
- âœ… **Data Validation** - Check duplicate sebelum download
- âœ… **Environment Variables** - Secure credential management
- âœ… **Backup System** - Auto backup saat migration

---

## ğŸš€ Quick Start

### 1. Setup Environment

```powershell
# Clone repository
git clone https://github.com/ocidserver/crawlSeruti.git
cd crawlSeruti

# Install dependencies (using venv)
.venv\Scripts\python.exe -m pip install -r requirements.txt
```

### 2. Configure

```powershell
# Copy environment file
Copy-Item .env.example .env

# Edit .env file
notepad .env
```

**Required Configuration:**

```env
# BPS SSO Credentials
USERNAME=your_username
PASSWORD=your_password

# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-here
FLASK_PORT=5000
FLASK_DEBUG=True

# Download Settings
DOWNLOAD_PATH=downloads
HEADLESS_MODE=True
```

### 3. Run Application

```powershell
# Start Flask server
.venv\Scripts\python.exe run.py

# Access web interface
# Open browser: http://localhost:5000
```

---

## ğŸ“– Documentation

### ğŸ“š User Guides

- [Getting Started Guide](docs/GETTING_STARTED.md) - Panduan lengkap untuk pemula
- [Seruti Crawler Guide](docs/SERUTI_GUIDE.md) - Panduan crawler Seruti
- [Susenas Crawler Guide](docs/SUSENAS_GUIDE.md) - Panduan crawler Susenas
- [Scheduler Guide](docs/SCHEDULER_GUIDE.md) - Panduan task scheduler

### ğŸ”§ Technical Docs

- [Architecture](docs/ARCHITECTURE.md) - System architecture & design
- [Database Schema](docs/DATABASE.md) - SQLite schema & queries
- [API Reference](docs/API.md) - REST API endpoints

### ğŸ“ Change History

- [CHANGELOG.md](docs/CHANGELOG.md) - Complete version history
- [Migration Guide](docs/MIGRATION.md) - Upgrade instructions

---

## ğŸ—ï¸ Architecture

```
crawlSeruti/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py           # Flask app factory
â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”œâ”€â”€ database.py           # SQLite database manager
â”‚   â”œâ”€â”€ scheduler.py          # APScheduler wrapper
â”‚   â”œâ”€â”€ download_log.py       # Download tracking
â”‚   â”œâ”€â”€ routes.py             # API endpoints
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ base_crawler.py   # Abstract base class
â”‚   â”‚   â”œâ”€â”€ seruti_crawler.py # Seruti implementation
â”‚   â”‚   â””â”€â”€ susenas_crawler.py# Susenas implementation
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html        # Web dashboard
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ tests/                    # Test files
â”œâ”€â”€ downloads/                # Downloaded files
â”œâ”€â”€ crawler.db                # SQLite database
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ run.py                    # Application entry point
```

---

## ğŸ® Usage

### Web Dashboard

1. **Start Server**

   ```powershell
   .venv\Scripts\python.exe run.py
   ```

2. **Open Dashboard**

   - URL: http://localhost:5000
   - Select crawler: SERUTI atau SUSENAS
   - Configure schedule
   - Add job

3. **Monitor Jobs**
   - View active/inactive jobs in table format
   - Check status, next run, last run
   - Cancel jobs if needed
   - View download log

### Command Line

**Manual Crawl:**

```powershell
# Seruti crawler
.venv\Scripts\python.exe -c "from app.crawlers import get_crawler; from app.config import Config; crawler = get_crawler('seruti')(Config.USERNAME, Config.PASSWORD); print(crawler.run())"

# Susenas crawler
.venv\Scripts\python.exe -c "from app.crawlers import get_crawler; from app.config import Config; crawler = get_crawler('susenas')(Config.USERNAME, Config.PASSWORD); print(crawler.run())"
```

**Run Tests:**

```powershell
# Test Seruti
.venv\Scripts\python.exe tests\test_multi_crawler.py

# Test Susenas
.venv\Scripts\python.exe tests\test_susenas_crawl.py
```

---

## ğŸ“Š Crawlers

### 1. Seruti Crawler

- **Target:** https://olah.web.bps.go.id
- **Function:** Download Progres Triwulan
- **Authentication:** SSO BPS
- **Data Format:** Excel (.xlsx)
- **Performance:** ~57 seconds per download
- **Optimization:** 11.5% faster vs original

### 2. Susenas Crawler

- **Target:** https://webmonitoring.bps.go.id/sen
- **Function:** Download 7 progress reports
- **Reports:**
  1. Laporan Pencacahan
  2. Laporan Pemeriksaan (Edcod)
  3. Laporan Pengiriman ke Kabkot
  4. Laporan Penerimaan di Kabkot
  5. Laporan Penerimaan di IPDS
  6. Laporan Pengolahan Dokumen M
  7. Laporan Pengolahan Dokumen KP
- **Performance:** ~40 seconds for 7 files (headless)
- **Smart Validation:** Skip if data already exists (29 seconds)

---

## ğŸ—„ï¸ Database

### Tables

**scheduled_jobs:**

```sql
CREATE TABLE scheduled_jobs (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    crawler_type TEXT NOT NULL,
    start_date TEXT NOT NULL,
    end_date TEXT NOT NULL,
    hour INTEGER NOT NULL,
    minute INTEGER NOT NULL,
    status TEXT DEFAULT 'active',
    created_at TEXT NOT NULL,
    last_run TEXT,
    last_message TEXT
);
```

**download_logs:**

```sql
CREATE TABLE download_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    nama_file TEXT NOT NULL,
    tanggal_download TEXT NOT NULL,
    laman_web TEXT NOT NULL,
    data_tanggal TEXT,
    task_name TEXT DEFAULT 'Manual'
);
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable           | Description           | Default   |
| ------------------ | --------------------- | --------- |
| `USERNAME`         | BPS SSO username      | -         |
| `PASSWORD`         | BPS SSO password      | -         |
| `FLASK_SECRET_KEY` | Flask session key     | -         |
| `FLASK_PORT`       | Web server port       | 5000      |
| `FLASK_DEBUG`      | Debug mode            | False     |
| `DOWNLOAD_PATH`    | Download directory    | downloads |
| `HEADLESS_MODE`    | Browser headless mode | True      |

### Scheduler Configuration

| Parameter     | Description                     | Range      |
| ------------- | ------------------------------- | ---------- |
| `start_date`  | Job start date                  | YYYY-MM-DD |
| `end_date`    | Job end date                    | YYYY-MM-DD |
| `hour`        | Execution hour                  | 0-23       |
| `minute`      | Execution minute                | 0-59       |
| `max_retries` | Max retry attempts              | 0-10       |
| `retry_delay` | Delay between retries (seconds) | 60+        |

---

## ğŸ§ª Testing

### Run All Tests

```powershell
# Test multi-crawler
.venv\Scripts\python.exe tests\test_multi_crawler.py

# Test Susenas crawler
.venv\Scripts\python.exe tests\test_susenas_crawl.py

# Test download detection
.venv\Scripts\python.exe tests\test_download_detection.py
```

### Test Results

- âœ… Seruti: Download successful (57s)
- âœ… Susenas: 7/7 files downloaded (40s)
- âœ… Smart validation: Skip duplicate (29s)
- âœ… Database: ACID transactions working
- âœ… UI: Table format responsive

---

## ğŸ› Troubleshooting

### ChromeDriver Issues

```powershell
# Fix ChromeDriver path
.venv\Scripts\python.exe fix_chromedriver.py

# Clear cache
Remove-Item -Recurse -Force "$env:USERPROFILE\.wdm"
```

### Database Issues

```bash
# Check database
sqlite3 crawler.db ".tables"

# Backup database
sqlite3 crawler.db ".backup crawler_backup.db"

# Re-migrate from JSON
rm crawler.db
# Restore *.json.backup files
# Restart server (auto-migration)
```

### Server Not Starting

```powershell
# Check port availability
netstat -ano | findstr :5000

# Use different port
$env:FLASK_PORT="5001"
.venv\Scripts\python.exe run.py
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

## ğŸ“œ License

This project is for internal BPS use only.

---

## ğŸ‘¥ Authors

- **IPDS-OCID Team** - BPS Provinsi Kepulauan Riau

---

## ğŸ™ Acknowledgments

- BPS IT Team untuk SSO infrastructure
- APScheduler untuk job scheduling
- Selenium WebDriver untuk browser automation
- Flask untuk web framework
- SQLite untuk embedded database

---

## ğŸ“ Support

Untuk bantuan dan pertanyaan:

- Email: [your-email@bps.go.id]
- Issue Tracker: [GitHub Issues](https://github.com/ocidserver/crawlSeruti/issues)

---

**Last Updated:** November 2025  
**Version:** 2.0.0  
**Status:** Production Ready âœ…
